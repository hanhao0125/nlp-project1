{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# word2vec的实现\n",
    "资料可参考 reference material。\n",
    "主要参考知乎 https://zhuanlan.zhihu.com/p/26306795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 首先实现基于 skip gram 的模型。\n",
    " skip-gram 最基本的方法是使用当前词预测下一个词。\n",
    " 其训练本质是一个分类问题，假设词表大小是V,那么输入就是V维 one-hot 向量，经过一个隐藏层，维度为N,然后做一个V分类。\n",
    " 可以看出如果直接优化这个任务，会非常难以训练。现实中 V 可能会很大，从而导致很难训练。word2vec 用了一些训练 trick，如*hierarchical softmax*,这里先不考虑，只考虑最简单的。\n",
    " ![abc](image/skip-grim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9448/9448 [00:11<00:00, 801.85it/s]\n",
      "  0%|          | 0/9448 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.611 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "100%|██████████| 9448/9448 [00:09<00:00, 895.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from langconv import *\n",
    "file_path = os.getcwd() + '/extracted/AA/'\n",
    "file_names = os.listdir(file_path)\n",
    "def token(string):\n",
    "    return ' '.join(re.findall('[\\w|\\d]+', string))\n",
    "# 转换繁体到简体\n",
    "def cht_to_chs(line):\n",
    "    line = Converter('zh-hans').convert(line)\n",
    "    line.encode('utf-8')\n",
    "    return line\n",
    "\n",
    "all_articles = []\n",
    "# 数据量过大 仅用2个200M一共400M数据来构建\n",
    "for file_name in file_names[:1]:\n",
    "    with open(file_path+file_name, encoding='utf-8') as fo:\n",
    "        for article in tqdm(fo.readlines()):\n",
    "            all_articles.append(cht_to_chs(token(json.loads(article)['text'].strip())))\n",
    "\n",
    "def cut(str): \n",
    "    result = list(jieba.cut(str))\n",
    "    return result\n",
    "\n",
    "sentences = []\n",
    "for s in tqdm(all_articles):\n",
    "    sentences.append(cut(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97219\n",
      "19767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97219"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "min_count = 5\n",
    "cnt = Counter(sum(sentences,[]))\n",
    "print(len(cnt))\n",
    "f = list(filter(lambda x:x[1] >= min_count,cnt.items()))\n",
    "f = [x[0] for x in f]\n",
    "print(len(f))\n",
    "\n",
    "sentences = sentences\n",
    "tokens = set()\n",
    "for s in sentences:\n",
    "    tokens = tokens | set(s)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9448/9448 [01:56<00:00, 81.44it/s]\n"
     ]
    }
   ],
   "source": [
    "word2index = {w:i for i,w in enumerate(f)}\n",
    "index2word = {i:w for w,i in word2index.items()}\n",
    "X = []\n",
    "Y = []\n",
    "window = 2\n",
    "for s in tqdm(sentences):\n",
    "    for i in range(window,len(s) - window):\n",
    "        if s[i] not in f:continue\n",
    "        X.append([word2index.get(k,0) for k in s[i-window:i]] \n",
    "                 + [word2index.get(k,0) for k in s[i+1:i+window+1]])\n",
    "        Y.append(word2index[s[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952502\n",
      "19767\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952502\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class Sample(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.LongTensor(x)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "sn = len(X) \n",
    "dset = Sample(X[:sn],Y[:sn])\n",
    "loader = DataLoader(dset,batch_size=5000,shuffle=False)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "token_size = len(word2index)\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self,vsz=token_size,emd=100,cls_num=token_size):\n",
    "        super(Word2Vec,self).__init__()\n",
    "        self.e = nn.Embedding(vsz,emd)\n",
    "#         self.hidden = nn.Linear(emd*4,emd)\n",
    "        self.clf = nn.Linear(emd,19767)\n",
    "        self.e.weight.data.uniform_(-0.1,0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.e(x)\n",
    "        x = torch.mean(x,1)\n",
    "        x = self.clf(x)\n",
    "        return x \n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.e = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.e(inputs).view((inputs.shape[0], -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(\n",
      "  (e): Embedding(19767, 100)\n",
      "  (clf): Linear(in_features=100, out_features=19767, bias=True)\n",
      ")\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:1: Loss = 7.074193370279842:   0%|          | 0/100 [00:10<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:1: Loss = 7.074193370279842:   1%|          | 1/100 [00:10<17:28, 10.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:2: Loss = 5.887467705142436:   1%|          | 1/100 [00:20<17:28, 10.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:2: Loss = 5.887467705142436:   2%|▏         | 2/100 [00:20<17:06, 10.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:3: Loss = 5.34063166348722:   2%|▏         | 2/100 [00:32<17:06, 10.47s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:3: Loss = 5.34063166348722:   3%|▎         | 3/100 [00:32<17:23, 10.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:4: Loss = 4.897340150403727:   3%|▎         | 3/100 [00:44<17:23, 10.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:4: Loss = 4.897340150403727:   4%|▍         | 4/100 [00:44<17:44, 11.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:5: Loss = 4.515165931891397:   4%|▍         | 4/100 [00:55<17:44, 11.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:5: Loss = 4.515165931891397:   5%|▌         | 5/100 [00:55<17:48, 11.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:6: Loss = 4.185933231683301:   5%|▌         | 5/100 [01:06<17:48, 11.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:6: Loss = 4.185933231683301:   6%|▌         | 6/100 [01:06<17:21, 11.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:7: Loss = 3.905058108075127:   6%|▌         | 6/100 [01:17<17:21, 11.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:7: Loss = 3.905058108075127:   7%|▋         | 7/100 [01:17<17:24, 11.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:8: Loss = 3.6718946289641696:   7%|▋         | 7/100 [01:29<17:24, 11.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:8: Loss = 3.6718946289641696:   8%|▊         | 8/100 [01:29<17:08, 11.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:9: Loss = 3.481868080443737:   8%|▊         | 8/100 [01:39<17:08, 11.18s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:9: Loss = 3.481868080443737:   9%|▉         | 9/100 [01:39<16:47, 11.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:10: Loss = 3.3272155610678706:   9%|▉         | 9/100 [01:50<16:47, 11.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:10: Loss = 3.3272155610678706:  10%|█         | 10/100 [01:50<16:22, 10.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:11: Loss = 3.2005934946199988:  10%|█         | 10/100 [02:02<16:22, 10.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:11: Loss = 3.2005934946199988:  11%|█         | 11/100 [02:02<16:31, 11.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:12: Loss = 3.096485171642603:  11%|█         | 11/100 [02:12<16:31, 11.15s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:12: Loss = 3.096485171642603:  12%|█▏        | 12/100 [02:12<16:05, 10.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:13: Loss = 3.0081005346088507:  12%|█▏        | 12/100 [02:23<16:05, 10.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:13: Loss = 3.0081005346088507:  13%|█▎        | 13/100 [02:23<15:55, 10.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:14: Loss = 2.9334382717522027:  13%|█▎        | 13/100 [02:35<15:55, 10.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:14: Loss = 2.9334382717522027:  14%|█▍        | 14/100 [02:35<15:58, 11.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:15: Loss = 2.8704090062236287:  14%|█▍        | 14/100 [02:45<15:58, 11.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:15: Loss = 2.8704090062236287:  15%|█▌        | 15/100 [02:45<15:16, 10.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:16: Loss = 2.815738031377343:  15%|█▌        | 15/100 [02:54<15:16, 10.78s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:16: Loss = 2.815738031377343:  16%|█▌        | 16/100 [02:54<14:18, 10.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:17: Loss = 2.76923827663142:  16%|█▌        | 16/100 [03:02<14:18, 10.22s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:17: Loss = 2.76923827663142:  17%|█▋        | 17/100 [03:02<13:36,  9.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:18: Loss = 2.7279426190241467:  17%|█▋        | 17/100 [03:12<13:36,  9.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:18: Loss = 2.7279426190241467:  18%|█▊        | 18/100 [03:12<13:29,  9.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:19: Loss = 2.692302620847812:  18%|█▊        | 18/100 [03:22<13:29,  9.87s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:19: Loss = 2.692302620847812:  19%|█▉        | 19/100 [03:22<13:15,  9.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:20: Loss = 2.6601119490818204:  19%|█▉        | 19/100 [03:32<13:15,  9.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:20: Loss = 2.6601119490818204:  20%|██        | 20/100 [03:32<13:08,  9.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:21: Loss = 2.6322764950897057:  20%|██        | 20/100 [03:42<13:08,  9.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:21: Loss = 2.6322764950897057:  21%|██        | 21/100 [03:42<13:08,  9.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:22: Loss = 2.606172377526448:  21%|██        | 21/100 [03:52<13:08,  9.98s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:22: Loss = 2.606172377526448:  22%|██▏       | 22/100 [03:52<12:57,  9.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:23: Loss = 2.5829099104666584:  22%|██▏       | 22/100 [04:02<12:57,  9.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:23: Loss = 2.5829099104666584:  23%|██▎       | 23/100 [04:02<12:35,  9.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:24: Loss = 2.5610905380149163:  23%|██▎       | 23/100 [04:11<12:35,  9.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:24: Loss = 2.5610905380149163:  24%|██▍       | 24/100 [04:11<12:15,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:25: Loss = 2.5422542126390946:  24%|██▍       | 24/100 [04:21<12:15,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:25: Loss = 2.5422542126390946:  25%|██▌       | 25/100 [04:21<12:10,  9.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:26: Loss = 2.5236137025643393:  25%|██▌       | 25/100 [04:31<12:10,  9.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:26: Loss = 2.5236137025643393:  26%|██▌       | 26/100 [04:31<12:04,  9.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:27: Loss = 2.5080681071855633:  26%|██▌       | 26/100 [04:41<12:04,  9.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:27: Loss = 2.5080681071855633:  27%|██▋       | 27/100 [04:41<11:57,  9.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:28: Loss = 2.4917599012714406:  27%|██▋       | 27/100 [04:51<11:57,  9.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:28: Loss = 2.4917599012714406:  28%|██▊       | 28/100 [04:51<11:53,  9.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:29: Loss = 2.4786610946605343:  28%|██▊       | 28/100 [05:01<11:53,  9.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:29: Loss = 2.4786610946605343:  29%|██▉       | 29/100 [05:01<11:57, 10.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:30: Loss = 2.464572239920731:  29%|██▉       | 29/100 [05:12<11:57, 10.10s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:30: Loss = 2.464572239920731:  30%|███       | 30/100 [05:12<11:47, 10.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:31: Loss = 2.4536580390331006:  30%|███       | 30/100 [05:22<11:47, 10.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:31: Loss = 2.4536580390331006:  31%|███       | 31/100 [05:22<11:41, 10.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:32: Loss = 2.4409727176446565:  31%|███       | 31/100 [05:32<11:41, 10.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:32: Loss = 2.4409727176446565:  32%|███▏      | 32/100 [05:32<11:33, 10.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:33: Loss = 2.4310987420107057:  32%|███▏      | 32/100 [05:42<11:33, 10.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:33: Loss = 2.4310987420107057:  33%|███▎      | 33/100 [05:42<11:19, 10.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:34: Loss = 2.4199710272993715:  33%|███▎      | 33/100 [05:51<11:19, 10.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:34: Loss = 2.4199710272993715:  34%|███▍      | 34/100 [05:51<10:53,  9.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:35: Loss = 2.4112815245283836:  34%|███▍      | 34/100 [06:01<10:53,  9.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:35: Loss = 2.4112815245283836:  35%|███▌      | 35/100 [06:01<10:37,  9.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:36: Loss = 2.401502867643746:  35%|███▌      | 35/100 [06:10<10:37,  9.80s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:36: Loss = 2.401502867643746:  36%|███▌      | 36/100 [06:10<10:19,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:37: Loss = 2.393661175722851:  36%|███▌      | 36/100 [06:20<10:19,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:37: Loss = 2.393661175722851:  37%|███▋      | 37/100 [06:20<10:05,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:38: Loss = 2.384383401321491:  37%|███▋      | 37/100 [06:29<10:05,  9.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:38: Loss = 2.384383401321491:  38%|███▊      | 38/100 [06:29<09:50,  9.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:39: Loss = 2.377847456183109:  38%|███▊      | 38/100 [06:39<09:50,  9.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:39: Loss = 2.377847456183109:  39%|███▉      | 39/100 [06:39<09:40,  9.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:40: Loss = 2.3693512322390893:  39%|███▉      | 39/100 [06:48<09:40,  9.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:40: Loss = 2.3693512322390893:  40%|████      | 40/100 [06:48<09:27,  9.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:41: Loss = 2.363509059576464:  40%|████      | 40/100 [06:57<09:27,  9.46s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:41: Loss = 2.363509059576464:  41%|████      | 41/100 [06:57<09:16,  9.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:42: Loss = 2.355750483992212:  41%|████      | 41/100 [07:07<09:16,  9.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:42: Loss = 2.355750483992212:  42%|████▏     | 42/100 [07:07<09:10,  9.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:43: Loss = 2.3499594084255357:  42%|████▏     | 42/100 [07:16<09:10,  9.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:43: Loss = 2.3499594084255357:  43%|████▎     | 43/100 [07:16<08:58,  9.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:44: Loss = 2.3420512233105004:  43%|████▎     | 43/100 [07:26<08:58,  9.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:44: Loss = 2.3420512233105004:  44%|████▍     | 44/100 [07:26<08:46,  9.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:45: Loss = 2.336053421360036:  44%|████▍     | 44/100 [07:35<08:46,  9.41s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:45: Loss = 2.336053421360036:  45%|████▌     | 45/100 [07:35<08:37,  9.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:46: Loss = 2.329355627454388:  45%|████▌     | 45/100 [07:45<08:37,  9.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:46: Loss = 2.329355627454388:  46%|████▌     | 46/100 [07:45<08:29,  9.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:47: Loss = 2.325191134557674:  46%|████▌     | 46/100 [07:54<08:29,  9.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:47: Loss = 2.325191134557674:  47%|████▋     | 47/100 [07:54<08:17,  9.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:48: Loss = 2.320105596986741:  47%|████▋     | 47/100 [08:03<08:17,  9.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:48: Loss = 2.320105596986741:  48%|████▊     | 48/100 [08:03<08:06,  9.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:49: Loss = 2.3155082008601484:  48%|████▊     | 48/100 [08:13<08:06,  9.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:49: Loss = 2.3155082008601484:  49%|████▉     | 49/100 [08:13<07:58,  9.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:50: Loss = 2.3102412386090343:  49%|████▉     | 49/100 [08:22<07:58,  9.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:50: Loss = 2.3102412386090343:  50%|█████     | 50/100 [08:22<07:49,  9.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:51: Loss = 2.306358253144469:  50%|█████     | 50/100 [08:32<07:49,  9.38s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:51: Loss = 2.306358253144469:  51%|█████     | 51/100 [08:32<07:44,  9.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:52: Loss = 2.301366193132251:  51%|█████     | 51/100 [08:41<07:44,  9.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:52: Loss = 2.301366193132251:  52%|█████▏    | 52/100 [08:41<07:36,  9.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:53: Loss = 2.2972270506214723:  52%|█████▏    | 52/100 [08:51<07:36,  9.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:53: Loss = 2.2972270506214723:  53%|█████▎    | 53/100 [08:51<07:30,  9.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:54: Loss = 2.291493143086658:  53%|█████▎    | 53/100 [09:01<07:30,  9.59s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:54: Loss = 2.291493143086658:  54%|█████▍    | 54/100 [09:01<07:22,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:55: Loss = 2.2876732736357845:  54%|█████▍    | 54/100 [09:10<07:22,  9.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:55: Loss = 2.2876732736357845:  55%|█████▌    | 55/100 [09:10<07:13,  9.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:56: Loss = 2.283050072754865:  55%|█████▌    | 55/100 [09:20<07:13,  9.64s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:56: Loss = 2.283050072754865:  56%|█████▌    | 56/100 [09:20<07:07,  9.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:57: Loss = 2.279765644622723:  56%|█████▌    | 56/100 [09:30<07:07,  9.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:57: Loss = 2.279765644622723:  57%|█████▋    | 57/100 [09:30<06:56,  9.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:58: Loss = 2.275608500260957:  57%|█████▋    | 57/100 [09:40<06:56,  9.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:58: Loss = 2.275608500260957:  58%|█████▊    | 58/100 [09:40<06:47,  9.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:59: Loss = 2.2732974640361925:  58%|█████▊    | 58/100 [09:49<06:47,  9.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:59: Loss = 2.2732974640361925:  59%|█████▉    | 59/100 [09:49<06:36,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:60: Loss = 2.2695125185382303:  59%|█████▉    | 59/100 [10:00<06:36,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:60: Loss = 2.2695125185382303:  60%|██████    | 60/100 [10:00<06:34,  9.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:61: Loss = 2.266294083670172:  60%|██████    | 60/100 [10:10<06:34,  9.85s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:61: Loss = 2.266294083670172:  61%|██████    | 61/100 [10:10<06:25,  9.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:62: Loss = 2.2622865088947157:  61%|██████    | 61/100 [10:20<06:25,  9.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:62: Loss = 2.2622865088947157:  62%|██████▏   | 62/100 [10:20<06:22, 10.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:63: Loss = 2.2594020123257064:  62%|██████▏   | 62/100 [10:30<06:22, 10.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:63: Loss = 2.2594020123257064:  63%|██████▎   | 63/100 [10:30<06:13, 10.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:64: Loss = 2.2547921042167705:  63%|██████▎   | 63/100 [10:40<06:13, 10.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:64: Loss = 2.2547921042167705:  64%|██████▍   | 64/100 [10:40<05:57,  9.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:65: Loss = 2.253183427281405:  64%|██████▍   | 64/100 [10:49<05:57,  9.93s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:65: Loss = 2.253183427281405:  65%|██████▌   | 65/100 [10:49<05:41,  9.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:66: Loss = 2.2501754423710687:  65%|██████▌   | 65/100 [10:58<05:41,  9.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:66: Loss = 2.2501754423710687:  66%|██████▌   | 66/100 [10:58<05:27,  9.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:67: Loss = 2.2482859264493613:  66%|██████▌   | 66/100 [11:08<05:27,  9.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:67: Loss = 2.2482859264493613:  67%|██████▋   | 67/100 [11:08<05:16,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:68: Loss = 2.2449799276771345:  67%|██████▋   | 67/100 [11:17<05:16,  9.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:68: Loss = 2.2449799276771345:  68%|██████▊   | 68/100 [11:17<05:04,  9.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:69: Loss = 2.2428651374047965:  68%|██████▊   | 68/100 [11:27<05:04,  9.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:69: Loss = 2.2428651374047965:  69%|██████▉   | 69/100 [11:27<04:53,  9.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:70: Loss = 2.23933852093382:  69%|██████▉   | 69/100 [11:36<04:53,  9.46s/it]  \u001b[A\u001b[A\n",
      "\n",
      "Epoch:70: Loss = 2.23933852093382:  70%|███████   | 70/100 [11:36<04:46,  9.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:71: Loss = 2.2372926174034:  70%|███████   | 70/100 [11:46<04:46,  9.54s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:71: Loss = 2.2372926174034:  71%|███████   | 71/100 [11:46<04:33,  9.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:72: Loss = 2.234164263565503:  71%|███████   | 71/100 [11:55<04:33,  9.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:72: Loss = 2.234164263565503:  72%|███████▏  | 72/100 [11:55<04:24,  9.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:73: Loss = 2.2320471858478967:  72%|███████▏  | 72/100 [12:05<04:24,  9.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:73: Loss = 2.2320471858478967:  73%|███████▎  | 73/100 [12:05<04:15,  9.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:74: Loss = 2.229515105017817:  73%|███████▎  | 73/100 [12:14<04:15,  9.46s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:74: Loss = 2.229515105017817:  74%|███████▍  | 74/100 [12:14<04:07,  9.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:75: Loss = 2.227417372284135:  74%|███████▍  | 74/100 [12:24<04:07,  9.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:75: Loss = 2.227417372284135:  75%|███████▌  | 75/100 [12:24<03:57,  9.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:76: Loss = 2.2244764460319:  75%|███████▌  | 75/100 [12:33<03:57,  9.52s/it]  \u001b[A\u001b[A\n",
      "\n",
      "Epoch:76: Loss = 2.2244764460319:  76%|███████▌  | 76/100 [12:33<03:49,  9.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:77: Loss = 2.2222686007384853:  76%|███████▌  | 76/100 [12:43<03:49,  9.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:77: Loss = 2.2222686007384853:  77%|███████▋  | 77/100 [12:43<03:42,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:78: Loss = 2.2203250386952105:  77%|███████▋  | 77/100 [12:53<03:42,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:78: Loss = 2.2203250386952105:  78%|███████▊  | 78/100 [12:53<03:35,  9.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:79: Loss = 2.218400408460208:  78%|███████▊  | 78/100 [13:04<03:35,  9.78s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:79: Loss = 2.218400408460208:  79%|███████▉  | 79/100 [13:04<03:28,  9.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:80: Loss = 2.2166915607702045:  79%|███████▉  | 79/100 [13:14<03:28,  9.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:80: Loss = 2.2166915607702045:  80%|████████  | 80/100 [13:14<03:18,  9.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:81: Loss = 2.2151262504268066:  80%|████████  | 80/100 [13:24<03:18,  9.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:81: Loss = 2.2151262504268066:  81%|████████  | 81/100 [13:24<03:09,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:82: Loss = 2.2129537940649464:  81%|████████  | 81/100 [13:34<03:09,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:82: Loss = 2.2129537940649464:  82%|████████▏ | 82/100 [13:34<02:59, 10.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:83: Loss = 2.2129730785080275:  82%|████████▏ | 82/100 [13:44<02:59, 10.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:83: Loss = 2.2129730785080275:  83%|████████▎ | 83/100 [13:44<02:49,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:84: Loss = 2.2103653852852228:  83%|████████▎ | 83/100 [13:54<02:49,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:84: Loss = 2.2103653852852228:  84%|████████▍ | 84/100 [13:54<02:40, 10.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:85: Loss = 2.2081215275519805:  84%|████████▍ | 84/100 [14:04<02:40, 10.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:85: Loss = 2.2081215275519805:  85%|████████▌ | 85/100 [14:04<02:31, 10.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:86: Loss = 2.2047772544840867:  85%|████████▌ | 85/100 [14:14<02:31, 10.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:86: Loss = 2.2047772544840867:  86%|████████▌ | 86/100 [14:14<02:21, 10.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:87: Loss = 2.202583205637507:  86%|████████▌ | 86/100 [14:24<02:21, 10.09s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:87: Loss = 2.202583205637507:  87%|████████▋ | 87/100 [14:24<02:10, 10.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:88: Loss = 2.1999958260521213:  87%|████████▋ | 87/100 [14:34<02:10, 10.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:88: Loss = 2.1999958260521213:  88%|████████▊ | 88/100 [14:34<02:01, 10.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:89: Loss = 2.199161897779135:  88%|████████▊ | 88/100 [14:44<02:01, 10.11s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:89: Loss = 2.199161897779135:  89%|████████▉ | 89/100 [14:44<01:50, 10.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:90: Loss = 2.1984679243327436:  89%|████████▉ | 89/100 [14:54<01:50, 10.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:90: Loss = 2.1984679243327436:  90%|█████████ | 90/100 [14:54<01:39,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:91: Loss = 2.1983878930825838:  90%|█████████ | 90/100 [15:04<01:39,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:91: Loss = 2.1983878930825838:  91%|█████████ | 91/100 [15:04<01:30, 10.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:92: Loss = 2.196499254691039:  91%|█████████ | 91/100 [15:14<01:30, 10.02s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:92: Loss = 2.196499254691039:  92%|█████████▏| 92/100 [15:14<01:20, 10.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:93: Loss = 2.1953653808663653:  92%|█████████▏| 92/100 [15:24<01:20, 10.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:93: Loss = 2.1953653808663653:  93%|█████████▎| 93/100 [15:24<01:09,  9.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:94: Loss = 2.1936472953926205:  93%|█████████▎| 93/100 [15:34<01:09,  9.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:94: Loss = 2.1936472953926205:  94%|█████████▍| 94/100 [15:34<00:59, 10.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:95: Loss = 2.19164366684659:  94%|█████████▍| 94/100 [15:44<00:59, 10.00s/it]  \u001b[A\u001b[A\n",
      "\n",
      "Epoch:95: Loss = 2.19164366684659:  95%|█████████▌| 95/100 [15:44<00:50, 10.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:96: Loss = 2.1887094406557335:  95%|█████████▌| 95/100 [15:54<00:50, 10.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:96: Loss = 2.1887094406557335:  96%|█████████▌| 96/100 [15:54<00:39,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:97: Loss = 2.188058519238577:  96%|█████████▌| 96/100 [16:04<00:39,  9.99s/it] \u001b[A\u001b[A\n",
      "\n",
      "Epoch:97: Loss = 2.188058519238577:  97%|█████████▋| 97/100 [16:04<00:29,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:98: Loss = 2.1858515876750046:  97%|█████████▋| 97/100 [16:14<00:29,  9.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:98: Loss = 2.1858515876750046:  98%|█████████▊| 98/100 [16:14<00:20, 10.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:99: Loss = 2.1847465025816915:  98%|█████████▊| 98/100 [16:24<00:20, 10.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:99: Loss = 2.1847465025816915:  99%|█████████▉| 99/100 [16:24<00:10, 10.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:100: Loss = 2.1836261362305485:  99%|█████████▉| 99/100 [16:34<00:10, 10.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Epoch:100: Loss = 2.1836261362305485: 100%|██████████| 100/100 [16:34<00:00, 10.03s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle as pkl\n",
    "from torch.autograd import Variable\n",
    "epoch = 100 \n",
    "# m = CBOW(len(word2index),100,2,64)\n",
    "m = Word2Vec()\n",
    "m = m.cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(m.parameters(),lr=0.01)\n",
    "l = []\n",
    "print(m)\n",
    "print('start training...')\n",
    "def train():\n",
    "    pb = tqdm(range(epoch))\n",
    "    for e in pb:\n",
    "        bl = [] \n",
    "        for b in loader:\n",
    "            bx,by = b[0],b[1]\n",
    "            bx,by = bx.cuda(),by.cuda()\n",
    "            optim.zero_grad()\n",
    "            o = m(bx)\n",
    "\n",
    "            loss = loss_func(o,by)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            bl.append(loss.item())\n",
    "        l.append(np.mean(bl))\n",
    "        pb.set_description(f'Epoch:{e+1}: Loss = {l[-1]}')\n",
    "#         print(f'epoch {e+1} : {l[-1]}')\n",
    "train()\n",
    "with open('./saved_files/e.pkl','wb') as f:\n",
    "    pkl.dump(m.e,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim 数据接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec # word2vec 模型\n",
    "w2v_model = word2vec.Word2Vec(sentences,min_count=5,workers=50,size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19767, 100)\n",
      "[('动物', 1.0), ('网路', 0.45104304), ('脊椎动物', 0.42363852), ('Club', 0.40397677), ('证书', 0.3995529), ('锻焊', 0.39632577), ('瓦', 0.3944327), ('御', 0.3888452), ('劳动力', 0.3882849), ('俩', 0.38493794)]\n",
      "[('M8802', 0.9665425419807434), ('常见', 0.9658806920051575), ('两种', 0.9552369713783264), ('鸟类', 0.9476338028907776), ('灭绝', 0.9474760293960571), ('分类', 0.9388299584388733), ('原始', 0.937654972076416), ('属实', 0.9372254014015198), ('骨骼', 0.9371610283851624), ('特征', 0.9354689717292786)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def similar(v1,v2): \n",
    "    return np.dot(v1,v2) / (np.linalg.norm(v1)*(np.linalg.norm(v2)))\n",
    "\n",
    "e = None\n",
    "\n",
    "with open('./saved_files/e.pkl','rb') as f:\n",
    "    e = pkl.load(f)\n",
    "    e = e.weight.detach().cpu().numpy()\n",
    "# e = normalize(e)\n",
    "q = '动物'\n",
    "print(e.shape)\n",
    "s = {}\n",
    "for w,i in word2index.items():\n",
    "    s[w] = similar(e[i],e[word2index[q]])\n",
    "s = sorted(s.items(),key=lambda x:x[1],reverse=True)[:10]\n",
    "print(s)\n",
    "print(w2v_model.wv.most_similar(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19767"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
