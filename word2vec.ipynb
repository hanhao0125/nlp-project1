{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# word2vec的实现\n",
    "资料可参考 reference material。\n",
    "主要参考知乎 https://zhuanlan.zhihu.com/p/26306795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 首先实现基于 skip gram 的模型。\n",
    " skip-gram 最基本的方法是使用当前词预测下一个词。\n",
    " 其训练本质是一个分类问题，假设词表大小是V,那么输入就是V维 one-hot 向量，经过一个隐藏层，维度为N,然后做一个V分类。\n",
    " 可以看出如果直接优化这个任务，会非常难以训练。现实中 V 可能会很大，从而导致很难训练。word2vec 用了一些训练 trick，如*hierarchical softmax*,这里先不考虑，只考虑最简单的。\n",
    " ![abc](image/skip-grim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9448/9448 [00:11<00:00, 653.36it/s]\n",
      "100%|██████████| 9448/9448 [00:08<00:00, 1106.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from langconv import *\n",
    "file_path = os.getcwd() + '/extracted/AA/'\n",
    "file_names = os.listdir(file_path)\n",
    "def token(string):\n",
    "    return ' '.join(re.findall('[\\w|\\d]+', string))\n",
    "# 转换繁体到简体\n",
    "def cht_to_chs(line):\n",
    "    line = Converter('zh-hans').convert(line)\n",
    "    line.encode('utf-8')\n",
    "    return line\n",
    "\n",
    "all_articles = []\n",
    "# 数据量过大 仅用2个200M一共400M数据来构建\n",
    "for file_name in file_names[:1]:\n",
    "    with open(file_path+file_name, encoding='utf-8') as fo:\n",
    "        for article in tqdm(fo.readlines()):\n",
    "            all_articles.append(cht_to_chs(token(json.loads(article)['text'].strip())))\n",
    "\n",
    "def cut(str): \n",
    "    result = list(jieba.cut(str))\n",
    "    return result\n",
    "\n",
    "sentences = []\n",
    "for s in tqdm(all_articles):\n",
    "    sentences.append(cut(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97219"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只取 5000 个句子\n",
    "sentences = sentences\n",
    "tokens = set()\n",
    "for s in sentences:\n",
    "    tokens = tokens | set(s)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w:i for i,w in enumerate(tokens)}\n",
    "index2word = {i:w for w,i in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9448/9448 [00:02<00:00, 3508.82it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "window = 2\n",
    "for s in tqdm(sentences):\n",
    "    for i in range(window,len(s) - window):\n",
    "        X.append([word2index[k] for k in s[i-window:i]] \n",
    "                 + [word2index[k] for k in s[i+1:i+window+1]])\n",
    "        Y.append(word2index[s[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class Sample(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.LongTensor(x)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "dset = Sample(X,Y)\n",
    "loader = DataLoader(dset,batch_size=3000,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "token_size = len(word2index)\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self,vsz=token_size,emd=100,cls_num=token_size):\n",
    "        super(Word2Vec,self).__init__()\n",
    "        self.e = nn.Embedding(vsz,emd)\n",
    "        self.hidden = nn.Linear(emd*4,emd)\n",
    "        self.clf = nn.Linear(emd,cls_num)\n",
    "        self.e.weight.data.uniform_(-0.1,0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.e(x)\n",
    "#         x = torch.sum(x,1)\n",
    "        x = x.view((x.shape[0],-1))\n",
    "        x = self.hidden(x)\n",
    "        x = self.clf(x)\n",
    "        return x \n",
    "    \n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(\n",
      "  (e): Embedding(97219, 100)\n",
      "  (hidden): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (clf): Linear(in_features=100, out_features=97219, bias=True)\n",
      ")\n",
      "start training...\n",
      "epoch 1 : 3528.148344039917\n",
      "epoch 2 : 3167.2226452827454\n",
      "epoch 3 : 3027.433641910553\n",
      "epoch 4 : 2931.926839351654\n",
      "epoch 5 : 2864.992835521698\n",
      "epoch 6 : 2811.6909680366516\n",
      "epoch 7 : 2766.0571932792664\n",
      "epoch 8 : 2726.707187652588\n",
      "epoch 9 : 2693.8013215065002\n",
      "epoch 10 : 2666.2686433792114\n",
      "epoch 11 : 2642.6601638793945\n",
      "epoch 12 : 2622.0766978263855\n",
      "epoch 13 : 2604.0904779434204\n",
      "epoch 14 : 2588.291410923004\n",
      "epoch 15 : 2574.218727350235\n",
      "epoch 16 : 2561.470993757248\n",
      "epoch 17 : 2549.7606110572815\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle as pkl\n",
    "from torch.autograd import Variable\n",
    "epoch = 50\n",
    "# m = CBOW(len(word2index),100,2,64)\n",
    "m = Word2Vec()\n",
    "m = m.cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(m.parameters(),lr=0.1,mo)\n",
    "l = []\n",
    "print(m)\n",
    "print('start training...')\n",
    "def train():\n",
    "    for e in range(epoch):\n",
    "        bl = 0 \n",
    "        for b in loader:\n",
    "            bx,by = b[0],b[1]\n",
    "            bx,by = Variable(bx).cuda(),Variable(by).cuda()\n",
    "            optim.zero_grad()\n",
    "            o = m(bx)\n",
    "\n",
    "            loss = loss_func(o,by)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            bl += loss.item()\n",
    "        l.append(bl)\n",
    "        print(f'epoch {e+1} : {l[-1]}')\n",
    "train()\n",
    "with open('./saved_files/e.pkl','wb') as f:\n",
    "    pkl.dump(m.e,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1363', 0.45734346), ('卢思满', 0.45113978), ('越来越', 0.4503107), ('哈普沙', 0.44950783), ('A75', 0.44781357), ('配音', 0.4465693), ('贲张', 0.44448572), ('学位证书', 0.4442458), ('references', 0.4418396), ('枪枝', 0.44071117)]\n"
     ]
    }
   ],
   "source": [
    "def similar(v1,v2): \n",
    "    return np.dot(v1,v2) / np.sqrt(np.square(v1).sum()) * np.sqrt(np.square(v2).sum())\n",
    "e = None\n",
    "with open('./saved_files/e.pkl','rb') as f:\n",
    "    e = pkl.load(f)\n",
    "    e = e.weight.detach().cpu().numpy()\n",
    "w = '说'\n",
    "s = {}\n",
    "for w,i in word2index.items():\n",
    "    s[w] = similar(e[i],e[word2index[w]])\n",
    "s = sorted(s.items(),key=lambda x:x[1],reverse=True)\n",
    "print(s[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim 数据接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('家村', 0.9634177684783936),\n",
       " ('山村', 0.9586061835289001),\n",
       " ('西村', 0.9522800445556641),\n",
       " ('周家', 0.9470921158790588),\n",
       " ('东村', 0.9461714029312134),\n",
       " ('塘村', 0.9391768574714661),\n",
       " ('花园村', 0.9317656755447388),\n",
       " ('枫树', 0.927894115447998),\n",
       " ('新村', 0.8992545008659363),\n",
       " ('湖村', 0.8913074135780334)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec # word2vec 模型\n",
    "w2v_model = word2vec.Word2Vec(sentences,min_count=5,workers=50,size=100)\n",
    "w2v_model.wv.most_similar('村')\n",
    "print(similar(w2v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
